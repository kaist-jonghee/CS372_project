Hi I’m Jonghee Jeon From Team eighteen. Today, I will be presenting my solo project, ‘Lecture Summarization’. And this is the index of my presentation. In the present world, new information is rapidly flowing in. And it’s hard to capture them at a glance. But fortunately, most methods for conveying information help the users to do so. For example, newspapers have ‘titles’ that summarize the whole context. However, ‘Lecturing’ doesn’t provide any. And even though lecture slides may be provided, it’s hard to know which part was emphasized during the lecture. Therefore, I performed ‘Lecture Summarization’ that summarizes subtitles of the lecture into 20% of its original size. Here is the basic idea of my algorithm. Using the systematically-organized property of lectures, I first tried to split the text by subtopics. Then, for summarization, I’ve looked at 6 aspects of the text, where some of the aspects are related to the previous segmentation. And according to this, I gave corresponding scores to every sentence. Finally, chose the top-scored sentences and concatenated them to make a summary. The details will be explained shortly. But before diving right into the algorithm, I had to preprocess the text. And thus, I’ve performed sentence tokenization, word tokenization, and finally pronoun resolution. And to be more specific for pronoun resolution, I’ve resolved these pronouns. And the following are the key methods for resolving them. And next, to the main algorithm. First, I’ll be talking about segmentation. The key concept for this was to assume that segments are grouped by semantically similar sentences. Just like how we naturally section text. Now, this slide explains how I’ve actually implemented it. The main method I’ve used is sentence similarity. First, I’ve calculated the sentence similarity of the previous 2 sentences. And this is used as an indicator of how big it needs to be, to be called similar. Then, I’ve calculated the similarity between the target sentence and the previous 2 sentences. If it was not similar as shown here, I’ve calculated the following 2 sentences. And if they were similar as shown, I’ve claimed that, the target sentence is a ‘Segment Starting Sentence’. And segmentation was done based on these sentences. Although only the ‘Segment Starting Sentence’ was explained here, I’ve declared ‘stop sentences’ and ‘weird sentences’ too using sentence similarities. Then, how did I calculate sentence similarity? By using vocabulary sets and by using synonym set path similarities. Let’s look at an example. For the first method, the similarity was calculated by dividing the number of intersecting vocabulary by the size of the bigger vocabulary. And for the second method, I’ve averaged up every possible pair’s path_similarity. And, the second similarity function was chosen to be used. Next, is summarization. For every sentence in the text, I scored them according to the 6 aspects explained before. Here, instead of adding the same score for each aspect, I’ve rated the importance of each in the range of zero to one and used it as the addition factor for the corresponding aspect. The first and second aspect is whether it’s the start or the end of the segment. These were used because usually the key points are written in the first or last part of each segment. The next aspect is entropy. Here, the higher the entropy is, the more informative it is. The fourth and fifth aspect is whether it’s a question-asking sentence, or whether it’s a question-answering sentence. I’ve included them because usually questions are used to emphasize important concepts. And lastly, I checked whether each sentence is long enough. Because usually, too short sentences aren’t informative enough. And the upper right equation shows how to calculate entropy. Finally, I’ve chosen the high-scored sentences and used the exact same sentences for the summary. And in addition to the summary, I’ve found some name phrases from the 2 most high-entropy-valued sentences and used them to be Keywords. And showed them separately from the summary. Evaluation of the generated summary was done by ROUGE scores. It checks how many n-grams match with the n-grams in the truly labeled summaries. As the main metric, I chose ROUGE-1 f1 scores since there were many other studies using the same one, thus easy to compare the performance of my algorithm. And for the dataset, it was hard to find the dataset for the exact task that I was doing. So, I chose the CNN/Daily Mail dataset, which is a dataset that summarizes news articles. I thought that it was similar in the sense that lectures and news articles both convey information. First, with the training data of the dataset, I fine-tuned the weights used in scoring for summarization and got the following result. It implied that entropy wasn’t a good aspect to determine the importance of a sentence, the first sentence in each segment usually contained more important information than the last sentence, short sentences usually aren’t informative enough, and finally, that question-asking and question-answering contain important stuff. Then, with the test data of the dataset, I got these ROUGE scores. Here, the ROUGE-1 f1-score was 30.32% which was quite astonishing for me. Because it’s in the same 30’s with the leading deep-learning-models’ f1 scores. Also, the ROUGE-2 f1 score and ROUGE-L f1 score were high at 11.65% and 28.28% respectively.
